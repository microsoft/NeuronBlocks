

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>block_zoo.attentions package &mdash; NeuronBlocks 1.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> NeuronBlocks
          

          
          </a>

          
            
            
              <div class="version">
                1.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Basic block_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id15">Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id25">embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id27">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id31">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id38">Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id43">Encoder_Decoder</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id47">Normalizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html#id49">Loss Functions</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NeuronBlocks</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>block_zoo.attentions package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/block_zoo.attentions.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="block-zoo-attentions-package">
<h1>block_zoo.attentions package<a class="headerlink" href="#block-zoo-attentions-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-block_zoo.attentions.Attention">
<span id="block-zoo-attentions-attention-module"></span><h2>block_zoo.attentions.Attention module<a class="headerlink" href="#module-block_zoo.attentions.Attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.Attention.Attention">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.Attention.</code><code class="descname">Attention</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#Attention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.Attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>Attention layer</p>
<p>Given sequences X and Y, match sequence Y to each element in X.
* o_i = sum(alpha_j * y_j) for i in X
* alpha_j = softmax(y_j * x_i)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>layer_conf</strong> (<a class="reference internal" href="#block_zoo.attentions.Attention.AttentionConf" title="block_zoo.attentions.Attention.AttentionConf"><em>AttentionConf</em></a>) – configuration of a layer</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.Attention.Attention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>x_len</em>, <em>y</em>, <em>y_len</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#Attention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.Attention.forward" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>Tensor</em>) – [batch_size, x_max_len, dim].</li>
<li><strong>x_len</strong> (<em>Tensor</em>) – [batch_size], default is None.</li>
<li><strong>y</strong> (<em>Tensor</em>) – [batch_size, y_max_len, dim].</li>
<li><strong>y_len</strong> (<em>Tensor</em>) – [batch_size], default is None.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">has the same shape as x.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.Attention.AttentionConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.Attention.</code><code class="descname">AttentionConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#AttentionConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.AttentionConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<p>Configuration for Attention layer</p>
<dl class="method">
<dt id="block_zoo.attentions.Attention.AttentionConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#AttentionConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.AttentionConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.Attention.AttentionConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#AttentionConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.AttentionConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.Attention.AttentionConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#AttentionConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.AttentionConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.Attention.AttentionConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Attention.html#AttentionConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Attention.AttentionConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions.BiAttFlow">
<span id="block-zoo-attentions-biattflow-module"></span><h2>block_zoo.attentions.BiAttFlow module<a class="headerlink" href="#module-block_zoo.attentions.BiAttFlow" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlow">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.BiAttFlow.</code><code class="descname">BiAttFlow</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlow" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>implement AttentionFlow layer for BiDAF
[paper]: <a class="reference external" href="https://arxiv.org/pdf/1611.01603.pdf">https://arxiv.org/pdf/1611.01603.pdf</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>layer_conf</strong> (<em>AttentionFlowConf</em>) – configuration of the AttentionFlowConf</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlow.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>content</em>, <em>content_len</em>, <em>query</em>, <em>query_len=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlow.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlow.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>implement the attention flow layer of BiDAF model</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>(</strong><strong>Tensor</strong><strong>)</strong> (<em>query</em>) – [batch_size, content_seq_len, dim]</li>
<li><strong>content_len</strong> – [batch_size]</li>
<li><strong>(</strong><strong>Tensor</strong><strong>)</strong> – [batch_size, query_seq_len, dim]</li>
<li><strong>query_len</strong> – [batch_size]</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">the tensor has same shape as content</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlowConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.BiAttFlow.</code><code class="descname">BiAttFlowConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlowConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlowConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<p>Configuration for AttentionFlow layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>attention_dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – dropout rate of attention matrix dropout operation</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlowConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlowConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlowConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlowConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlowConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlowConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlowConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlowConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlowConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.BiAttFlow.BiAttFlowConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BiAttFlow.html#BiAttFlowConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BiAttFlow.BiAttFlowConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions.BilinearAttention">
<span id="block-zoo-attentions-bilinearattention-module"></span><h2>block_zoo.attentions.BilinearAttention module<a class="headerlink" href="#module-block_zoo.attentions.BilinearAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttention">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.BilinearAttention.</code><code class="descname">BilinearAttention</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>BilinearAttention layer for DrQA
[paper]  <a class="reference external" href="https://arxiv.org/abs/1704.00051">https://arxiv.org/abs/1704.00051</a>
[GitHub] <a class="reference external" href="https://github.com/facebookresearch/DrQA">https://github.com/facebookresearch/DrQA</a>
:param layer_conf: configuration of a layer
:type layer_conf: BilinearAttentionConf</p>
<dl class="method">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>x_len</em>, <em>y</em>, <em>y_len</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>process inputs</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>Tensor</em>) – [batch_size, x_len, x_dim].</li>
<li><strong>x_len</strong> (<em>Tensor</em>) – [batch_size], default is None.</li>
<li><strong>y</strong> (<em>Tensor</em>) – [batch_size, y_dim].</li>
<li><strong>y_len</strong> (<em>Tensor</em>) – [batch_size], default is None.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">[batch_size, x_len, 1].
x_len:</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttentionConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.BilinearAttention.</code><code class="descname">BilinearAttentionConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttentionConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttentionConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<p>Configuration for Bilinear attention layer</p>
<dl class="method">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttentionConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttentionConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttentionConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttentionConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttentionConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttentionConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttentionConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttentionConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttentionConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.BilinearAttention.BilinearAttentionConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/BilinearAttention.html#BilinearAttentionConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.BilinearAttention.BilinearAttentionConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions.FullAttention">
<span id="block-zoo-attentions-fullattention-module"></span><h2>block_zoo.attentions.FullAttention module<a class="headerlink" href="#module-block_zoo.attentions.FullAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.FullAttention.FullAttention">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.FullAttention.</code><code class="descname">FullAttention</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>Full-aware fusion of:
Via, U., With, T., &amp; To, P. (2018). Fusion Net: Fusing Via Fully-Aware Attention with Application to Machine Comprehension, 1–17.</p>
<dl class="method">
<dt id="block_zoo.attentions.FullAttention.FullAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>string1</em>, <em>string1_len</em>, <em>string2</em>, <em>string2_len</em>, <em>string1_HoW</em>, <em>string1_How_len</em>, <em>string2_HoW</em>, <em>string2_HoW_len</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>To get representation of string1, we use string1 and string2 to obtain attention weights and use string2 to represent string1</p>
<p>Note: actually, the semantic information of string1 is not used, we only need string1’s seq_len information</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>string1</strong> – [batch size, seq_len, input_dim1]</li>
<li><strong>string1_len</strong> – [batch_size]</li>
<li><strong>string2</strong> – [batch size, seq_len, input_dim2]</li>
<li><strong>string2_len</strong> – [batch_size]</li>
<li><strong>string1_HoW</strong> – [batch size, seq_len, att_dim1]</li>
<li><strong>string1_HoW_len</strong> – [batch_size]</li>
<li><strong>string2_HoW</strong> – [batch size, seq_len, att_dim2]</li>
<li><strong>string2_HoW_len</strong> – [batch_size]</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">string1’s representation
string1_len</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.FullAttention.FullAttentionConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.FullAttention.</code><code class="descname">FullAttentionConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttentionConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttentionConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<dl class="method">
<dt id="block_zoo.attentions.FullAttention.FullAttentionConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttentionConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttentionConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.FullAttention.FullAttentionConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttentionConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttentionConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.FullAttention.FullAttentionConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttentionConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttentionConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.FullAttention.FullAttentionConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttentionConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttentionConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.FullAttention.FullAttentionConf.verify_before_inference">
<code class="descname">verify_before_inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/FullAttention.html#FullAttentionConf.verify_before_inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.FullAttention.FullAttentionConf.verify_before_inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Some conditions must be fulfilled, otherwise there would be errors when calling inference()</p>
<dl class="docutils">
<dt>The difference between verify_before_inference() and verify() is that:</dt>
<dd>verify_before_inference() is called before inference() while verify() is called after inference().</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions.LinearAttention">
<span id="block-zoo-attentions-linearattention-module"></span><h2>block_zoo.attentions.LinearAttention module<a class="headerlink" href="#module-block_zoo.attentions.LinearAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.LinearAttention.LinearAttention">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.LinearAttention.</code><code class="descname">LinearAttention</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>Linear attention.
Combinate the original sequence along the sequence_length dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>layer_conf</strong> (<a class="reference internal" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf" title="block_zoo.attentions.LinearAttention.LinearAttentionConf"><em>LinearAttentionConf</em></a>) – configuration of a layer</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.LinearAttention.LinearAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>string</em>, <em>string_len=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>process inputs</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>string</strong> (<em>Variable</em>) – (batch_size, sequence_length, dim)</li>
<li><strong>string_len</strong> (<em>ndarray</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.7)"><em>None</em></a>) – [batch_size]</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>if keep_dim == False:</dt>
<dd><p class="first last">Output dimention: (batch_size, dim)</p>
</dd>
<dt>else:</dt>
<dd><p class="first last">just reweight along the sequence_length dimension: (batch_size, sequence_length, dim)</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Variable</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.LinearAttention.LinearAttentionConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.LinearAttention.</code><code class="descname">LinearAttentionConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttentionConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<p>Configuration for Linear attention layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>keep_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Whether to sum up the sequence representation along the sequence axis.
if False, the layer would return (batch_size, dim)
if True, the layer would keep the same dimension as input, thus return (batch_size, sequence_length, dim)</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.LinearAttention.LinearAttentionConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttentionConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.LinearAttention.LinearAttentionConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttentionConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.LinearAttention.LinearAttentionConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttentionConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.LinearAttention.LinearAttentionConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttentionConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.LinearAttention.LinearAttentionConf.verify_before_inference">
<code class="descname">verify_before_inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/LinearAttention.html#LinearAttentionConf.verify_before_inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.LinearAttention.LinearAttentionConf.verify_before_inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Some conditions must be fulfilled, otherwise there would be errors when calling inference()</p>
<dl class="docutils">
<dt>The difference between verify_before_inference() and verify() is that:</dt>
<dd>verify_before_inference() is called before inference() while verify() is called after inference().</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions.MatchAttention">
<span id="block-zoo-attentions-matchattention-module"></span><h2>block_zoo.attentions.MatchAttention module<a class="headerlink" href="#module-block_zoo.attentions.MatchAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.MatchAttention.MatchAttention">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.MatchAttention.</code><code class="descname">MatchAttention</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>MatchAttention layer for DrQA
[paper]  <a class="reference external" href="https://arxiv.org/abs/1704.00051">https://arxiv.org/abs/1704.00051</a>
[GitHub] <a class="reference external" href="https://github.com/facebookresearch/DrQA">https://github.com/facebookresearch/DrQA</a></p>
<p>Given sequences X and Y, match sequence Y to each element in X.
* o_i = sum(alpha_j * y_j) for i in X
* alpha_j = softmax(y_j * x_i)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>layer_conf</strong> (<a class="reference internal" href="#block_zoo.attentions.MatchAttention.MatchAttentionConf" title="block_zoo.attentions.MatchAttention.MatchAttentionConf"><em>MatchAttentionConf</em></a>) – configuration of a layer</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.MatchAttention.MatchAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>x_len</em>, <em>y</em>, <em>y_len</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> – [batch_size, x_max_len, dim].</li>
<li><strong>x_len</strong> – [batch_size], default is None.</li>
<li><strong>y</strong> – [batch_size, y_max_len, dim].</li>
<li><strong>y_len</strong> – [batch_size], default is None.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">has the same shape as x.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.MatchAttention.MatchAttentionConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.MatchAttention.</code><code class="descname">MatchAttentionConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttentionConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttentionConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<p>Configuration for MatchAttention layer</p>
<dl class="method">
<dt id="block_zoo.attentions.MatchAttention.MatchAttentionConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttentionConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttentionConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.MatchAttention.MatchAttentionConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttentionConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttentionConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.MatchAttention.MatchAttentionConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttentionConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttentionConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.MatchAttention.MatchAttentionConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/MatchAttention.html#MatchAttentionConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.MatchAttention.MatchAttentionConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions.Seq2SeqAttention">
<span id="block-zoo-attentions-seq2seqattention-module"></span><h2>block_zoo.attentions.Seq2SeqAttention module<a class="headerlink" href="#module-block_zoo.attentions.Seq2SeqAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttention">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.Seq2SeqAttention.</code><code class="descname">Seq2SeqAttention</code><span class="sig-paren">(</span><em>layer_conf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseLayer" title="block_zoo.BaseLayer.BaseLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseLayer</span></code></a></p>
<p>Linear layer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>layer_conf</strong> (<a class="reference internal" href="block_zoo.html#block_zoo.Linear.LinearConf" title="block_zoo.Linear.LinearConf"><em>LinearConf</em></a>) – configuration of a layer</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>string</em>, <em>string_len</em>, <em>string2</em>, <em>string2_len=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>utilize both string2 and string itself to generate attention weights to represent string.</dt>
<dd><dl class="first last docutils">
<dt>There are two steps:</dt>
<dd><ol class="first last arabic simple">
<li>get a string2 to string attention to represent string.</li>
<li>get a string to string attention to represent string it self.</li>
<li>merge the two representation above.</li>
</ol>
</dd>
</dl>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>string</strong> (<em>Variable</em>) – [batch_size, string_seq_len, dim].</li>
<li><strong>string_len</strong> (<em>ndarray</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.7)"><em>None</em></a>) – [batch_size], default is None.</li>
<li><strong>string2</strong> (<em>Variable</em>) – [batch_size, string2_seq_len, dim].</li>
<li><strong>string2_len</strong> (<em>ndarray</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.7)"><em>None</em></a>) – [batch_size], default is None.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">has the same shape as string.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Variable</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf">
<em class="property">class </em><code class="descclassname">block_zoo.attentions.Seq2SeqAttention.</code><code class="descname">Seq2SeqAttentionConf</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttentionConf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="block_zoo.html#block_zoo.BaseLayer.BaseConf" title="block_zoo.BaseLayer.BaseConf"><code class="xref py py-class docutils literal notranslate"><span class="pre">block_zoo.BaseLayer.BaseConf</span></code></a></p>
<p>Configuration for Seq2SeqAttention layer</p>
<dl class="method">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.declare">
<code class="descname">declare</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttentionConf.declare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.declare" title="Permalink to this definition">¶</a></dt>
<dd><p>Define things like “input_ranks” and “num_of_inputs”, which are certain with regard to your layer</p>
<blockquote>
<div><p>num_of_input is N(N&gt;0) means this layer accepts N inputs;</p>
<p>num_of_input is -1 means this layer accepts any number of inputs;</p>
<p>The rank here is not the same as matrix rank:</p>
<blockquote>
<div><p>For a scalar, its rank is 0;</p>
<p>For a vector, its rank is 1;</p>
<p>For a matrix, its rank is 2;</p>
<p>For a cube of numbers, its rank is 3.</p>
</div></blockquote>
<p>…
For instance, the rank of (batch size, sequence length, hidden_dim) is 3.</p>
<p>if num_of_input &gt; 0:</p>
<blockquote>
<div>len(input_ranks) should be equal to num_of_input</div></blockquote>
<p>elif num_of_input == -1:</p>
<blockquote>
<div>input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.</div></blockquote>
<p>NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.default">
<code class="descname">default</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttentionConf.default"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.default" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttentionConf.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.verify">
<code class="descname">verify</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/block_zoo/attentions/Seq2SeqAttention.html#Seq2SeqAttentionConf.verify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#block_zoo.attentions.Seq2SeqAttention.Seq2SeqAttentionConf.verify" title="Permalink to this definition">¶</a></dt>
<dd><p>Define some necessary varification for your layer when we define the model.</p>
<p>If you define your own layer and rewrite this funciton, please add “super(YourLayerConf, self).verify()” at the beginning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-block_zoo.attentions">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-block_zoo.attentions" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Quantus STCA team&lt;quantus_stca@microsoft.com&gt;

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>